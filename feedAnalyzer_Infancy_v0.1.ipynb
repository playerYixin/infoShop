{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec4c799-73de-4ee8-9427-4fd8d09c41a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "# @playerYixin\n",
    "\n",
    "# Mac: comment multiple lines = command + /\n",
    "# Target: Read and Analyze local data from RSS Feed Explorer App - NetNewsWire\n",
    "# --------\n",
    "# 1: Find the file path where NetNewsWire's local data is stored.\n",
    "# According to NetNewsWire's GitHub doc (https://github.com/Ranchero-Software/NetNewsWire/blob/main/Technotes/Accounts.markdown),\n",
    "# the path should be something like \"~/Library/Application Support/NetNewsWire/Accounts/\" On the Mac.\n",
    "# But it took me almost 6 hours to find the real one:\n",
    "# \"/Users/{userName}/Library/Containers/com.ranchero.NetNewsWire-Evergreen/Data/Library/Application Support/NetNewsWire/Accounts/{accountName}/\". FYI.\n",
    "# --------\n",
    "# 2: Target data file is \"DB.sqlite3\", which contains 10 tables: articles, statuses, authors, authorsLookup, search, search_content, search_docsize, search_segdir, search_segments, search_stat\n",
    "# Table \"articles\" contains each piece of article pushed by subscribed feeds, which new rows appended according to your refresh frequency.\n",
    "# Column names are: articleID, feedID, uniqueID, title, contentHTML, contentText, url, external URL, summary, imageURL, bannerImageURL, datePublished, searchRowID.\n",
    "# Table \"statuses\" contains tags (0/1) about read and starred articles.\n",
    "# Column names are: articleID, read, starred, dateArrived\n",
    "# other tables are about your searching behaviors and are not considerred in this task.\n",
    "# --------\n",
    "# 3: The original meta-data of your subscribed RSS feeds (\"FeedMetadata.plist\") can be found in the same path as \"DB.sqlite3\".\n",
    "# You may need it to categorize and filter the articles.\n",
    "# --------\n",
    "# 4: This code provides a simplest implementation to achieve my infoShop project's target. (https://github.com/playerYixin/infoShop)\n",
    "# You can modify it according to your taste of the information source.\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import datetime\n",
    "#import dateutil.tz as tz\n",
    "import plistlib\n",
    "import markdown\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f17f871-0b03-47db-8ca0-7e0054dd9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamp range of 24 hours (between two successive UTC+0) before {prev} day\n",
    "\n",
    "def getTimeRange(prev=0):\n",
    "    dtEnd = datetime.datetime.combine(datetime.date.today()-datetime.timedelta(prev), datetime.datetime.min.time())\n",
    "    tsEnd = dtEnd.replace(tzinfo=datetime.timezone.utc).timestamp()\n",
    "    # tsEnd = dtEnd.replace(tzinfo=tz.tzlocal()).timestamp()\n",
    "    dtStart = dtEnd - datetime.timedelta(1)\n",
    "    tsStart = dtStart.replace(tzinfo=datetime.timezone.utc).timestamp()\n",
    "    # tsStart = dtStart.replace(tzinfo=tz.tzlocal()).timestamp()\n",
    "    return int(tsStart), int(tsEnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45d549c5-6cc9-44c2-b385-9480fb1dea0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start timestamp: 1665100800\n",
      "end timestamp: 1665187200\n",
      "TODAY_TAG: 2022_10_07\n"
     ]
    }
   ],
   "source": [
    "# Return timestamp range in consideration\n",
    "\n",
    "PREV_DAYS = 0         # you can fetch ealier articles by modifying this parameter (int)\n",
    "\n",
    "dtRange = getTimeRange(PREV_DAYS)\n",
    "TODAY_TAG = datetime.datetime.fromtimestamp(dtRange[0]).strftime(\"%Y_%m_%d\")\n",
    "print(\"start timestamp: %s\" % dtRange[0])\n",
    "print(\"end timestamp: %s\" % dtRange[1])\n",
    "print(\"TODAY_TAG: %s\" % TODAY_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "521d160c-da5c-4954-9bc2-724eb7f5c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SQL connection to our SQLite database\n",
    "\n",
    "# # set path_root of database\n",
    "# path_root = \"./\"\n",
    "\n",
    "# use the local path of NetNewsWire's local database\n",
    "path_root = \"/Users/{userName}/Library/Containers/com.ranchero.NetNewsWire-Evergreen/Data/Library/Application Support/NetNewsWire/Accounts/2_iCloud/\"\n",
    "\n",
    "# connect to database DB.sqlite3\n",
    "con = sqlite3.connect(path_root + \"DB.sqlite3\")\n",
    "\n",
    "# set encoding rule especially when the data includs Chinese characters.\n",
    "cur = con.cursor()\n",
    "cur.execute('pragma encoding=UTF8')\n",
    "\n",
    "# # Use \"cursor.execute\" to fetch a single row, can be iterated over by row\n",
    "# row = cur.execute('SELECT title FROM articles LIMIT 1;')\n",
    "# row = row.fetchall()\n",
    "\n",
    "# Use pandas.read_sql_query to fetch the whole table\n",
    "# Do not include column named \"contentHTML\" to avoid pandas parsing it incorrectly\n",
    "# column named \"searchRowID\" is also omitted here\n",
    "column_select = \"articleID, feedID, uniqueID, title, contentText, url, externalURL, summary, imageURL, datePublished\"\n",
    "\n",
    "df_article = pd.read_sql_query(\"SELECT %s FROM articles WHERE datePublished >= %s AND datePublished < %s ORDER BY datePublished DESC \" % (column_select,dtRange[0],dtRange[1]), con, parse_dates=['datePublished'])\n",
    "\n",
    "df_statuses = pd.read_sql_query(\"SELECT * FROM statuses WHERE dateArrived >= %s AND dateArrived < %s ORDER BY dateArrived DESC\" % (dtRange[0],dtRange[1]), con, parse_dates=['dateArrived'])\n",
    "\n",
    "# Be sure to close the connection\n",
    "con.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c3a637-b060-451b-aeed-7d3983058721",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4811d8be-7478-4100-9624-62b8086c500b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_statuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfb6fcd7-b094-4d86-9200-581ca5fc4213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                               0\n",
      "articleID                       0ef8a5abf01b9da9e919aeb57706d2aa\n",
      "feedID                             https://cointelegraph.com/rss\n",
      "uniqueID       https://cointelegraph.com/news/why-the-us-is-o...\n",
      "title          Why the US is one of the most crypto-friendly ...\n",
      "contentText                                                 None\n",
      "url                                                         None\n",
      "externalURL    https://cointelegraph.com/news/why-the-us-is-o...\n",
      "summary                                                     None\n",
      "imageURL                                                    None\n",
      "datePublished                                2022-10-07 23:52:58\n",
      "read                                                         NaN\n",
      "starred                                                      NaN\n",
      "dateArrived                                                  NaT\n",
      "\n",
      "Length of aticles: 157\n",
      "Length of statuses: 122\n",
      "Length of merged result: 157\n"
     ]
    }
   ],
   "source": [
    "# merge Table article and Table statuses on their common column named \"articleID\", print the length of the result\n",
    "\n",
    "df_merge = df_article.merge(df_statuses,left_on='articleID', right_on='articleID',how='left')\n",
    "\n",
    "print(df_merge.head(1).T)\n",
    "print(\"\\nLength of aticles: %s\" % len(df_article))\n",
    "print(\"Length of statuses: %s\" % len(df_statuses))\n",
    "print(\"Length of merged result: %s\" % len(df_merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89e80e07-b834-467a-b3d2-af7164fcca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if column 'externalURL' is NaN, fill it using column 'url''s value in the same row\n",
    "\n",
    "df_merge['externalURL'] = df_merge[['url','externalURL']].fillna(method=\"ffill\",axis=1)['externalURL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c1ca486-1ed6-409d-99de-25ba3293bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and set output path\n",
    "\n",
    "path_output = \"./output\"\n",
    "isExist = os.path.exists(path_output)\n",
    "\n",
    "if not isExist:\n",
    "    # Create a new directory because it does not exist \n",
    "    os.makedirs(path_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1ee8894-0037-477e-942a-1dc3cf0bf65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result to .csv file (optional)\n",
    "\n",
    "df_merge.to_csv('%s/NetNewsWire_DB_%s.csv' % (path_output, TODAY_TAG),encoding='utf_8_sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9556b4-fcf4-453f-b9da-c1549d4b328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you get the raw data from NetNewsWire's local DB. You can start analyze it by yourself.\n",
    "# Below I share an example how I process the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7521c36-b9e2-4c14-b4e9-e1e1b18e4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before analyzing the articles, you'd better select your favorite feeds and add some tags to these sources.\n",
    "# So that you can select articles from these feeds and read them in the first place.\n",
    "# ~~~~~~~~\n",
    "# In my case, I selected my favorite feeds and catergorized them into 6 groups:\n",
    "# ---\n",
    "# myAnalyst\n",
    "# myProject\n",
    "# myMedia\n",
    "# myPodcasts\n",
    "# myTracer\n",
    "# myKOL\n",
    "# ---\n",
    "# I also add tags \"mustRead\" and \"readTitlesOnly\" to some of these selected feeds to remind me how to treat them according to my own reading strategy.\n",
    "# ~~~~~~~~\n",
    "# As a result, you will get a cleaned table of feeds' mata data\n",
    "# You can get the raw feedMetaData from NetNewsWire's local file \"FeedMetadata.plist\" and edit based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18961f9f-9f37-4260-b12c-1f0baf4705de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# # This step show how I create my tagged feedMetaData file.\n",
    "# # If you have already created a \"feedMetaDataTagged.csv\" file, and NO NEW feeds are subscribed afterwords, skip this\n",
    "# # Anytime you add a new feed to NetNewsWire, you need to append it to the table.\n",
    "# # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# # 1.fetch unique list of subscribed feeds\n",
    "# with open(path_root + \"FeedMetadata.plist\", 'rb') as fp:\n",
    "#     pl = plistlib.load(fp)\n",
    "# feedMetaData = pd.DataFrame.from_dict(pl,orient='index')\n",
    "# feedMetaData.reset_index(inplace=True)\n",
    "\n",
    "# feedList = feedMetaData['feedID'].tolist()\n",
    "# print(\"Total number of feeds: %s\" % len(feedList))\n",
    "\n",
    "# # 2.save latest feedMetaData \n",
    "# feedMetaData.to_csv('./feedMetaData_%s.csv' % TODAY_TAG,encoding='utf_8_sig',index=False)\n",
    "\n",
    "# # 3.1(First, Manually) Add tags to feeds and save as a new file named \"feedMetaDataTagged.csv\"\n",
    "# # 3.2(Manually) Copy information related to the new feeds to \"feedMetaDataTagged.csv\" and add tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81733cee-adc8-4f38-b0e1-4739b17da89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing feed(s):\n",
      "['https://www.blocknative.com/blog/tag/defi/rss.xml']\n",
      "\n",
      "\n",
      "The personal tags are:\n",
      "['myKOL', 'myProject', 'myAnalyst', 'myPodcast', 'myTracer', 'myMedia']\n"
     ]
    }
   ],
   "source": [
    "# load tagged feeds' meta data\n",
    "df_feed = pd.read_csv('./feedMetaDataTagged.csv')\n",
    "\n",
    "# fetch unique list of subscribed feeds\n",
    "with open(path_root + \"FeedMetadata.plist\", 'rb') as fp:\n",
    "    pl = plistlib.load(fp)\n",
    "feedMetaData = pd.DataFrame.from_dict(pl,orient='index')\n",
    "feedMetaData.reset_index(inplace=True)\n",
    "\n",
    "# check if there's any new feed not recorded in \"feedMetaDataTagged.csv\"\n",
    "feed_diff = list(set(feedMetaData['feedID'].to_list()).difference(set(df_feed['feedID'].to_list())))\n",
    "if len(feed_diff)==0:\n",
    "    print('There no missing feed.\\n')\n",
    "else:\n",
    "    print('Missing feed(s):')\n",
    "    print(feed_diff)\n",
    "    print('\\n')\n",
    "\n",
    "# get Manual-added tag list except for 'mustRead' and 'readTitleOnly'\n",
    "feedTag = list(set(list(df_feed.columns)).difference(set(list(feedMetaData.columns)+['mustRead','readTitleOnly'])))\n",
    "print('The personal tags are:\\n%s' % feedTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9892314-0fc0-4fe7-9084-873cfcdf6b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>articleID</th>\n",
       "      <td>0ef8a5abf01b9da9e919aeb57706d2aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feedID</th>\n",
       "      <td>https://cointelegraph.com/rss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniqueID</th>\n",
       "      <td>https://cointelegraph.com/news/why-the-us-is-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>Why the US is one of the most crypto-friendly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contentText</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>externalURL</th>\n",
       "      <td>https://cointelegraph.com/news/why-the-us-is-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imageURL</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datePublished</th>\n",
       "      <td>2022-10-07 23:52:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>read</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starred</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dateArrived</th>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>editedName</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subscriptionID</th>\n",
       "      <td>420520888dc1754660a32a2971f2829d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homePageURL</th>\n",
       "      <td>https://cointelegraph.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mustRead</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>readTitleOnly</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>myAnalyst</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>myProject</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>myMedia</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>myPodcast</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>myTracer</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>myKOL</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                0\n",
       "articleID                        0ef8a5abf01b9da9e919aeb57706d2aa\n",
       "feedID                              https://cointelegraph.com/rss\n",
       "uniqueID        https://cointelegraph.com/news/why-the-us-is-o...\n",
       "title           Why the US is one of the most crypto-friendly ...\n",
       "contentText                                                  None\n",
       "url                                                          None\n",
       "externalURL     https://cointelegraph.com/news/why-the-us-is-o...\n",
       "summary                                                      None\n",
       "imageURL                                                     None\n",
       "datePublished                                 2022-10-07 23:52:58\n",
       "read                                                          NaN\n",
       "starred                                                       NaN\n",
       "dateArrived                                                   NaT\n",
       "editedName                                                    NaN\n",
       "subscriptionID                   420520888dc1754660a32a2971f2829d\n",
       "homePageURL                            https://cointelegraph.com/\n",
       "mustRead                                                        0\n",
       "readTitleOnly                                                   0\n",
       "myAnalyst                                                       0\n",
       "myProject                                                       0\n",
       "myMedia                                                         1\n",
       "myPodcast                                                       0\n",
       "myTracer                                                        0\n",
       "myKOL                                                           0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge article information and feed information into one table\n",
    "\n",
    "df_merge2 = df_merge.merge(df_feed,left_on='feedID', right_on='feedID',how='left')\n",
    "df_merge2.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d773f26-5828-4a47-aac6-468b714e7fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for digesting articles' information\n",
    "idx_list = []\n",
    "for ii in range(len(feedTag)):\n",
    "    idx_list.append(df_merge2[feedTag[ii]]==1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee31760d-ee7b-47d6-be02-22db23bf8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify articles according to tags and save to csv files\n",
    "columns_select = ['title','feedID','editedName','homePageURL','contentText','url','externalURL','datePublished','mustRead','readTitleOnly']\n",
    "\n",
    "for ii in range(len(feedTag)):\n",
    "    df_merge2.loc[idx_list[ii], columns_select].to_csv(\"%s/articles_%s_%s.csv\" % (path_output, feedTag[ii], TODAY_TAG), encoding='utf_8_sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "800ce3a1-9d4e-4188-8703-989b14cd257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the format of markdown file. Here emoji '&#x1F4CC;'(pin) means mustRead; emoji '&#x26A1; (flash) means readTitleOnly\n",
    "def writeRows(row, f):\n",
    "    mark1 = '&#x1F4CC;' if row['mustRead']==1 else ''\n",
    "    mark2 = '&#x26A1;' if row['readTitleOnly']==1 else ''\n",
    "    f.write(\"* {}{}[{}]({})&nbsp;*from*&nbsp;***{}***\\n\".format(mark1, mark2,row['title'],row['externalURL'],row['editedName']).encode('utf-8'))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5564be15-e91d-4400-b50b-2fc14f441bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the newsletter in markdown format\n",
    "NA_TAG = '*(N.A.)*'     # In case there's no article in certain category.\n",
    "\n",
    "with open('%s/Daily Reading List_%s.md' % (path_output, TODAY_TAG), 'bw+') as f:\n",
    "    \n",
    "    f.write('## {}_{}\\n'.format('Daily Reading List',TODAY_TAG).encode('utf-8'))\n",
    "    \n",
    "    tag = 'myTracer'\n",
    "    f.write('### {}\\n'.format('Industry Tracer').encode('utf-8'))\n",
    "    df_tmp = pd.read_csv(\"%s/articles_%s_%s.csv\" % (path_output, tag, TODAY_TAG)).sort_values(by=['mustRead','readTitleOnly'],ascending=False)\n",
    "    rows = df_tmp.to_dict('records')\n",
    "    if len(rows) == 0:\n",
    "        f.write('{}\\n'.format(NA_TAG).encode('utf-8'))\n",
    "    else:\n",
    "        for row in rows:\n",
    "            writeRows(row, f)\n",
    "        \n",
    "    tag = 'myAnalyst'\n",
    "    f.write('### {}\\n'.format('Deep Dive').encode('utf-8'))\n",
    "    df_tmp = pd.read_csv(\"%s/articles_%s_%s.csv\" % (path_output, tag, TODAY_TAG)).sort_values(by=['mustRead','readTitleOnly'],ascending=False)\n",
    "    rows = df_tmp.to_dict('records')\n",
    "    if len(rows) == 0:\n",
    "        f.write('{}\\n'.format(NA_TAG).encode('utf-8'))\n",
    "    else:\n",
    "        for row in rows:\n",
    "            writeRows(row, f)\n",
    "\n",
    "    tag = 'myPodcast'\n",
    "    f.write('### {}\\n'.format('Podcast').encode('utf-8'))\n",
    "    df_tmp = pd.read_csv(\"%s/articles_%s_%s.csv\" % (path_output, tag, TODAY_TAG)).sort_values(by=['mustRead','readTitleOnly'],ascending=False)\n",
    "    rows = df_tmp.to_dict('records')\n",
    "    if len(rows) == 0:\n",
    "        f.write('{}\\n'.format(NA_TAG).encode('utf-8'))\n",
    "    else:\n",
    "        for row in rows:\n",
    "            writeRows(row, f)\n",
    "            \n",
    "    tag = 'myProject'\n",
    "    f.write('### {}\\n'.format('Project Tracer').encode('utf-8'))\n",
    "    df_tmp = pd.read_csv(\"%s/articles_%s_%s.csv\" % (path_output, tag, TODAY_TAG)).sort_values(by=['mustRead','readTitleOnly'],ascending=False)\n",
    "    rows = df_tmp.to_dict('records')\n",
    "    if len(rows) == 0:\n",
    "        f.write('{}\\n'.format(NA_TAG).encode('utf-8'))\n",
    "    else:    \n",
    "        for row in rows:\n",
    "            writeRows(row, f)\n",
    "            \n",
    "    tag = 'myKOL'\n",
    "    f.write('### {}\\n'.format(\"KOL's Opinion\").encode('utf-8'))\n",
    "    df_tmp = pd.read_csv(\"%s/articles_%s_%s.csv\" % (path_output, tag, TODAY_TAG)).sort_values(by=['mustRead','readTitleOnly'],ascending=False)\n",
    "    rows = df_tmp.to_dict('records')\n",
    "    if len(rows) == 0:\n",
    "        f.write('{}\\n'.format(NA_TAG).encode('utf-8'))\n",
    "    else:\n",
    "        for row in rows:\n",
    "            writeRows(row, f)\n",
    "            \n",
    "    f.seek(0)\n",
    "    markdown.markdownFromFile(input=f, output='%s/Daily Reading List_%s.html' % (path_output, TODAY_TAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be868d0-c77a-498c-9da2-9a7190162535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
